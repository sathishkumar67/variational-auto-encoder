{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:06:21.947032Z","iopub.status.busy":"2024-05-27T10:06:21.946282Z","iopub.status.idle":"2024-05-27T10:06:45.218202Z","shell.execute_reply":"2024-05-27T10:06:45.216794Z","shell.execute_reply.started":"2024-05-27T10:06:21.946998Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'vae'...\n","remote: Enumerating objects: 20, done.\u001b[K\n","remote: Counting objects: 100% (16/16), done.\u001b[K\n","remote: Compressing objects: 100% (16/16), done.\u001b[K\n","remote: Total 20 (delta 5), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n","Unpacking objects: 100% (20/20), 6.44 KiB | 1.29 MiB/s, done.\n","Filtering content: 100% (3/3), 3.48 GiB | 170.00 MiB/s, done.\n"]}],"source":["!git clone https://huggingface.co/pt-sk/vae"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:06:45.220764Z","iopub.status.busy":"2024-05-27T10:06:45.220484Z","iopub.status.idle":"2024-05-27T10:06:45.535909Z","shell.execute_reply":"2024-05-27T10:06:45.534647Z","shell.execute_reply.started":"2024-05-27T10:06:45.220737Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"371ed6cb783d44578a43a357ef3ab113","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:01.504058Z","iopub.status.busy":"2024-05-27T10:07:01.503708Z","iopub.status.idle":"2024-05-27T10:07:01.508740Z","shell.execute_reply":"2024-05-27T10:07:01.507822Z","shell.execute_reply.started":"2024-05-27T10:07:01.504029Z"},"trusted":true},"outputs":[],"source":["# constatnt variables\n","IMG_DIR = \"/kaggle/input/flickrfaceshq-dataset-nvidia-resized-256px/resized\"\n","\n","IMG_SHAPE = (3, 256, 256)\n","BATCH_SIZE = 4\n","LEARNING_RATE = 1e-5\n","KLD_WEIGHT = 0.00025"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:08.636844Z","iopub.status.busy":"2024-05-27T10:07:08.636497Z","iopub.status.idle":"2024-05-27T10:07:13.960837Z","shell.execute_reply":"2024-05-27T10:07:13.959985Z","shell.execute_reply.started":"2024-05-27T10:07:08.636819Z"},"trusted":true},"outputs":[],"source":["# necessary libraries\n","import os\n","import math\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.io import read_image\n","from tqdm import tqdm\n","from torch.optim import AdamW\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:16.707581Z","iopub.status.busy":"2024-05-27T10:07:16.706626Z","iopub.status.idle":"2024-05-27T10:07:17.393977Z","shell.execute_reply":"2024-05-27T10:07:17.393128Z","shell.execute_reply.started":"2024-05-27T10:07:16.707541Z"},"trusted":true},"outputs":[],"source":["# creating a customimagedataset\n","class CustomImageDataset(Dataset):\n","    def __init__(self, img_dir):\n","        self.img_dir = img_dir\n","        self.img_labels = os.listdir(img_dir)\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n","        image = read_image(img_path)\n","\n","        return image/255.0\n","\n","# instansiate the class\n","custom_image_dataset = CustomImageDataset(IMG_DIR)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:22.904687Z","iopub.status.busy":"2024-05-27T10:07:22.904208Z","iopub.status.idle":"2024-05-27T10:07:22.909663Z","shell.execute_reply":"2024-05-27T10:07:22.908701Z","shell.execute_reply.started":"2024-05-27T10:07:22.904653Z"},"trusted":true},"outputs":[],"source":["# preparing dataloader\n","dataloader = DataLoader(custom_image_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:29.967455Z","iopub.status.busy":"2024-05-27T10:07:29.966611Z","iopub.status.idle":"2024-05-27T10:07:29.973549Z","shell.execute_reply":"2024-05-27T10:07:29.972656Z","shell.execute_reply.started":"2024-05-27T10:07:29.967409Z"},"trusted":true},"outputs":[{"data":{"text/plain":["17500"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len(dataloader)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:37.332084Z","iopub.status.busy":"2024-05-27T10:07:37.331735Z","iopub.status.idle":"2024-05-27T10:07:37.343577Z","shell.execute_reply":"2024-05-27T10:07:37.342642Z","shell.execute_reply.started":"2024-05-27T10:07:37.332054Z"},"trusted":true},"outputs":[],"source":["# self attention\n","class SelfAttention(nn.Module):\n","    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n","        super().__init__()\n","        # This combines the Wq, Wk and Wv matrices into one matrix\n","        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n","        # This one represents the Wo matrix\n","        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n","        self.n_heads = n_heads\n","        self.d_head = d_embed // n_heads\n","\n","    def forward(self, x, causal_mask=False):\n","        # x: # (Batch_Size, Seq_Len, Dim)\n","\n","        # (Batch_Size, Seq_Len, Dim)\n","        input_shape = x.shape \n","        \n","        # (Batch_Size, Seq_Len, Dim)\n","        batch_size, sequence_length, d_embed = input_shape \n","\n","        # (Batch_Size, Seq_Len, H, Dim / H)\n","        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head) \n","\n","        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n","        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n","        \n","        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n","        q = q.view(interim_shape).transpose(1, 2)\n","        k = k.view(interim_shape).transpose(1, 2)\n","        v = v.view(interim_shape).transpose(1, 2)\n","\n","        # (Batch_Size, H, Seq_Len, Dim) @ (Batch_Size, H, Dim, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n","        weight = q @ k.transpose(-1, -2)\n","        \n","        if causal_mask:\n","            # Mask where the upper triangle (above the principal diagonal) is 1\n","            mask = torch.ones_like(weight, dtype=torch.bool).triu(1) \n","            # Fill the upper triangle with -inf\n","            weight.masked_fill_(mask, -torch.inf) \n","        \n","        # Divide by d_k (Dim / H). \n","        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n","        weight /= math.sqrt(self.d_head) \n","\n","        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n","        weight = F.softmax(weight, dim=-1) \n","\n","        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n","        output = weight @ v\n","\n","        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n","        output = output.transpose(1, 2) \n","\n","        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n","        output = output.reshape(input_shape) \n","\n","        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n","        output = self.out_proj(output) \n","        \n","        # (Batch_Size, Seq_Len, Dim)\n","        return output"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:45.551039Z","iopub.status.busy":"2024-05-27T10:07:45.550679Z","iopub.status.idle":"2024-05-27T10:07:45.574369Z","shell.execute_reply":"2024-05-27T10:07:45.573512Z","shell.execute_reply.started":"2024-05-27T10:07:45.551009Z"},"trusted":true},"outputs":[],"source":["# Decoder\n","class VAE_AttentionBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.groupnorm = nn.GroupNorm(32, channels)\n","        self.attention = SelfAttention(1, channels)\n","    \n","    def forward(self, x):\n","        # x: (Batch_Size, Features, Height, Width)\n","\n","        residue = x \n","\n","        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n","        x = self.groupnorm(x)\n","\n","        n, c, h, w = x.shape\n","        \n","        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * Width)\n","        x = x.view((n, c, h * w))\n","        \n","        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Height * Width, Features). Each pixel becomes a feature of size \"Features\", the sequence length is \"Height * Width\".\n","        x = x.transpose(-1, -2)\n","        \n","        # Perform self-attention WITHOUT mask\n","        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n","        x = self.attention(x)\n","        \n","        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Features, Height * Width)\n","        x = x.transpose(-1, -2)\n","        \n","        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Features, Height, Width)\n","        x = x.view((n, c, h, w))\n","        \n","        # (Batch_Size, Features, Height, Width) + (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width) \n","        x += residue\n","\n","        # (Batch_Size, Features, Height, Width)\n","        return x \n","\n","class VAE_ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n","        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","\n","        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n","        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n","\n","        if in_channels == out_channels:\n","            self.residual_layer = nn.Identity()\n","        else:\n","            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n","    \n","    def forward(self, x):\n","        # x: (Batch_Size, In_Channels, Height, Width)\n","\n","        residue = x\n","\n","        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n","        x = self.groupnorm_1(x)\n","        \n","        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n","        x = F.silu(x)\n","        \n","        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n","        x = self.conv_1(x)\n","        \n","        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n","        x = self.groupnorm_2(x)\n","        \n","        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n","        x = F.silu(x)\n","        \n","        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n","        x = self.conv_2(x)\n","        \n","        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n","        return x + self.residual_layer(residue)\n","\n","class VAE_Decoder(nn.Sequential):\n","    def __init__(self):\n","        super().__init__(\n","            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n","            nn.Conv2d(4, 4, kernel_size=1, padding=0),\n","\n","            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            nn.Conv2d(4, 512, kernel_size=3, padding=1),\n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_AttentionBlock(512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # Repeats the rows and columns of the data by scale_factor (like when you resize an image by doubling its size).\n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 4, Width / 4)\n","            nn.Upsample(scale_factor=2),\n","            \n","            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1), \n","            \n","            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 2, Width / 2)\n","            nn.Upsample(scale_factor=2), \n","            \n","            # (Batch_Size, 512, Height / 2, Width / 2) -> (Batch_Size, 512, Height / 2, Width / 2)\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1), \n","            \n","            # (Batch_Size, 512, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n","            VAE_ResidualBlock(512, 256), \n","            \n","            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n","            VAE_ResidualBlock(256, 256), \n","            \n","            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n","            VAE_ResidualBlock(256, 256), \n","            \n","            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height, Width)\n","            nn.Upsample(scale_factor=2), \n","            \n","            # (Batch_Size, 256, Height, Width) -> (Batch_Size, 256, Height, Width)\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1), \n","            \n","            # (Batch_Size, 256, Height, Width) -> (Batch_Size, 128, Height, Width)\n","            VAE_ResidualBlock(256, 128), \n","            \n","            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n","            VAE_ResidualBlock(128, 128), \n","            \n","            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n","            VAE_ResidualBlock(128, 128), \n","            \n","            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n","            nn.GroupNorm(32, 128), \n","            \n","            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n","            nn.SiLU(), \n","            \n","            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 3, Height, Width)\n","            nn.Conv2d(128, 3, kernel_size=3, padding=1), \n","        )\n","\n","    def forward(self, x):\n","        # x: (Batch_Size, 4, Height / 8, Width / 8)\n","        \n","        # Remove the scaling added by the Encoder.\n","        x /= 0.18215\n","\n","        for module in self:\n","            x = module(x)\n","\n","        # (Batch_Size, 3, Height, Width)\n","        return x"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:52.119837Z","iopub.status.busy":"2024-05-27T10:07:52.119459Z","iopub.status.idle":"2024-05-27T10:07:52.134254Z","shell.execute_reply":"2024-05-27T10:07:52.133351Z","shell.execute_reply.started":"2024-05-27T10:07:52.119809Z"},"trusted":true},"outputs":[],"source":["# Encoder\n","class VAE_Encoder(nn.Sequential):\n","    def __init__(self):\n","        super().__init__(\n","            # (Batch_Size, Channel, Height, Width) -> (Batch_Size, 128, Height, Width)\n","            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n","            \n","             # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n","            VAE_ResidualBlock(128, 128),\n","            \n","            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n","            VAE_ResidualBlock(128, 128),\n","            \n","            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height / 2, Width / 2)\n","            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n","            \n","            # (Batch_Size, 128, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n","            VAE_ResidualBlock(128, 256), \n","            \n","            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n","            VAE_ResidualBlock(256, 256), \n","            \n","            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 4, Width / 4)\n","            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0), \n","            \n","            # (Batch_Size, 256, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n","            VAE_ResidualBlock(256, 512), \n","            \n","            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_AttentionBlock(512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            VAE_ResidualBlock(512, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            nn.GroupNorm(32, 512), \n","            \n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n","            nn.SiLU(), \n","\n","            # Because the padding=1, it means the width and height will increase by 2\n","            # Out_Height = In_Height + Padding_Top + Padding_Bottom\n","            # Out_Width = In_Width + Padding_Left + Padding_Right\n","            # Since padding = 1 means Padding_Top = Padding_Bottom = Padding_Left = Padding_Right = 1,\n","            # Since the Out_Width = In_Width + 2 (same for Out_Height), it will compensate for the Kernel size of 3\n","            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8). \n","            nn.Conv2d(512, 8, kernel_size=3, padding=1), \n","\n","            # (Batch_Size, 8, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8)\n","            nn.Conv2d(8, 8, kernel_size=1, padding=0), \n","        )\n","\n","    def forward(self, x):\n","        # x: (Batch_Size, Channel, Height, Width)\n","        # noise: (Batch_Size, 4, Height / 8, Width / 8)\n","\n","        for module in self:\n","\n","            if getattr(module, 'stride', None) == (2, 2):  # Padding at downsampling should be asymmetric (see #8)\n","                # Pad: (Padding_Left, Padding_Right, Padding_Top, Padding_Bottom).\n","                # Pad with zeros on the right and bottom.\n","                # (Batch_Size, Channel, Height, Width) -> (Batch_Size, Channel, Height + Padding_Top + Padding_Bottom, Width + Padding_Left + Padding_Right) = (Batch_Size, Channel, Height + 1, Width + 1)\n","                x = F.pad(x, (0, 1, 0, 1))\n","            \n","            x = module(x)\n","        # (Batch_Size, 8, Height / 8, Width / 8) -> two tensors of shape (Batch_Size, 4, Height / 8, Width / 8)\n","        mean, log_variance = torch.chunk(x, 2, dim=1)\n","        # Clamp the log variance between -30 and 20, so that the variance is between (circa) 1e-14 and 1e8. \n","        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n","        log_variance = torch.clamp(log_variance, -30, 20)\n","        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n","        variance = log_variance.exp()\n","        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n","        stdev = variance.sqrt()\n","        \n","        # getting noise\n","        noise = torch.randn_like(stdev)\n","        \n","        # Transform N(0, 1) -> N(mean, stdev) \n","        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n","        x = mean + stdev * noise\n","        \n","        # Scale by a constant\n","        # Constant taken from: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/configs/stable-diffusion/v1-inference.yaml#L17C1-L17C1\n","        x *= 0.18215\n","        \n","        return x, mean, log_variance"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:07:58.632674Z","iopub.status.busy":"2024-05-27T10:07:58.632182Z","iopub.status.idle":"2024-05-27T10:07:58.639826Z","shell.execute_reply":"2024-05-27T10:07:58.638908Z","shell.execute_reply.started":"2024-05-27T10:07:58.632640Z"},"trusted":true},"outputs":[],"source":["class Encoder_Decoder(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \n","    def forward(self, x):\n","        z, mean, log_variance = self.encoder(x)\n","        reconstructed_image = self.decoder(z)\n","        \n","        return {\n","            \"original_image\": x,\n","            \"latent_image\": z,\n","            \"reconstructed_image\": reconstructed_image,\n","            \"mean\": mean,\n","            \"log_variance\": log_variance\n","        }\n","    \n","    def encode(self, x):\n","        z, mean, log_variance = self.encoder(x)\n","        \n","        return z\n","    \n","    def decode(self, z):\n","        reconstructed_image = self.decoder(z)\n","        \n","        return reconstructed_image"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:08:05.616947Z","iopub.status.busy":"2024-05-27T10:08:05.616216Z","iopub.status.idle":"2024-05-27T10:08:06.751135Z","shell.execute_reply":"2024-05-27T10:08:06.750176Z","shell.execute_reply.started":"2024-05-27T10:08:05.616910Z"},"trusted":true},"outputs":[],"source":["# instansiating the classes\n","encoder = VAE_Encoder()\n","decoder = VAE_Decoder()\n","model = Encoder_Decoder(encoder, decoder).to(\"cuda\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:08:12.428978Z","iopub.status.busy":"2024-05-27T10:08:12.428643Z","iopub.status.idle":"2024-05-27T10:08:12.436673Z","shell.execute_reply":"2024-05-27T10:08:12.435730Z","shell.execute_reply.started":"2024-05-27T10:08:12.428954Z"},"trusted":true},"outputs":[],"source":["# optimizer and loss function\n","optimizer = AdamW(params=model.parameters(),\n","                 lr = LEARNING_RATE)\n","\n","def loss_function(model_output: dict):\n","    reconstruction_loss = F.mse_loss(model_output[\"reconstructed_image\"],\n","                                    model_output[\"original_image\"])\n","    \n","    mu, log_var = model_output[\"mean\"], model_output[\"log_variance\"]\n","    kld_loss = -0.5 * (1 + log_var - mu.pow(2) - log_var.exp()).mean()\n","    loss = reconstruction_loss + KLD_WEIGHT * kld_loss\n","    \n","    return loss"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:08:19.813279Z","iopub.status.busy":"2024-05-27T10:08:19.812664Z","iopub.status.idle":"2024-05-27T10:08:20.666287Z","shell.execute_reply":"2024-05-27T10:08:20.665497Z","shell.execute_reply.started":"2024-05-27T10:08:19.813248Z"},"trusted":true},"outputs":[],"source":["# loading the models and optimizer\n","state = torch.load(\"/kaggle/working/vae/Encoder_Decoder_2_EPOCH\")\n","model.load_state_dict(state[\"model_state_dict\"])\n","optimizer.load_state_dict(state[\"optimizer_state_dict\"])"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T10:08:29.793803Z","iopub.status.busy":"2024-05-27T10:08:29.793410Z","iopub.status.idle":"2024-05-27T14:04:54.900468Z","shell.execute_reply":"2024-05-27T14:04:54.899357Z","shell.execute_reply.started":"2024-05-27T10:08:29.793775Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training:  83%|████████▎ | 14490/17500 [3:15:45<40:38,  1.23it/s, train_loss=0.00372]  IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n","Training: 100%|██████████| 17500/17500 [3:56:25<00:00,  1.23it/s, train_loss=0.00207]\n"]}],"source":["EPOCHS = 1\n","for i in range(EPOCHS):\n","    iterator = tqdm(dataloader, desc=\"Training\", postfix={\"train_loss\":0.0})\n","    for item in iterator:\n","        model.train()\n","        item = item.to(\"cuda\")\n","        output = model(item)\n","        loss = loss_function(output)\n","        \n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # movving the data from gpu to cpu\n","        item = item.to(\"cpu\")\n","        output[\"original_image\"] = output[\"original_image\"].to(\"cpu\")\n","        output[\"latent_image\"] = output[\"latent_image\"].to(\"cpu\")\n","        output[\"reconstructed_image\"] = output[\"reconstructed_image\"].to(\"cpu\")\n","        output[\"mean\"] = output[\"mean\"].to(\"cpu\")\n","        output[\"log_variance\"] = output[\"log_variance\"].to(\"cpu\")\n","        loss = loss.to(\"cpu\")\n","        \n","        iterator.set_postfix({\"train_loss\": loss.item()}, refresh=False)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:11:10.837697Z","iopub.status.busy":"2024-05-27T14:11:10.837199Z","iopub.status.idle":"2024-05-27T14:11:12.245199Z","shell.execute_reply":"2024-05-27T14:11:12.244428Z","shell.execute_reply.started":"2024-05-27T14:11:10.837653Z"},"trusted":true},"outputs":[],"source":["torch.save({\n","    \"model_state_dict\" : model.state_dict(),\n","    \"optimizer_state_dict\": optimizer.state_dict()\n","}, 'Encoder_Decoder_2ndepo_ffhq.pt')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:11:34.474782Z","iopub.status.busy":"2024-05-27T14:11:34.474385Z","iopub.status.idle":"2024-05-27T14:12:14.765307Z","shell.execute_reply":"2024-05-27T14:12:14.764452Z","shell.execute_reply.started":"2024-05-27T14:11:34.474754Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95419fcb4570481e940c24dc9d7ae723","version_major":2,"version_minor":0},"text/plain":["Encoder_Decoder_2ndepo_ffhq.pt:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/pt-sk/vae/commit/e66660c05ce37ea35553d1c82023c7a899b4ee14', commit_message='Upload Encoder_Decoder_2ndepo_ffhq.pt with huggingface_hub', commit_description='', oid='e66660c05ce37ea35553d1c82023c7a899b4ee14', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import HfApi\n","api = HfApi()\n","api.upload_file(\n","    path_or_fileobj=\"/kaggle/working/Encoder_Decoder_2ndepo_ffhq.pt\",\n","    path_in_repo=\"Encoder_Decoder_2ndepo_ffhq.pt\",\n","    repo_id=\"pt-sk/vae\",\n","    repo_type=\"model\",\n",")"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T14:18:19.392742Z","iopub.status.busy":"2024-05-27T14:18:19.392373Z","iopub.status.idle":"2024-05-27T14:18:19.726373Z","shell.execute_reply":"2024-05-27T14:18:19.725195Z","shell.execute_reply.started":"2024-05-27T14:18:19.392709Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'dataloader' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdataloader\u001b[49m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(sample\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"]}],"source":["sample = next(iter(dataloader))[0]\n","plt.imshow(sample.permute(1, 2, 0))\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample = sample.unsqueeze(0)\n","z = model.encode(sample.to(\"cuda\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reconstructed_image = model.decode(z)\n","reconstructed_image = sample.squeeze(0).permute(1, 2, 0)\n","plt.imshow(reconstructed_image.detach().cpu().numpy())\n","plt.axis(\"off\");"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":497308,"sourceId":922869,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
